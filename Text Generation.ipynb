{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Character Level Text Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this exercise we will be generating text with the same writing style as the writer, the LSTM model has been trained from. This model will be trained in the charater-level which means that the sentence: \n",
    ">**\"The quick brown fox jumps over the lazy dog.\"**\n",
    "\n",
    "has a total of:\n",
    "\n",
    "```python\n",
    "sentence = list(\"The quick brown fox jumps over the lazy dog.\")\n",
    "print(sentence)\n",
    ">>>> ['T','h','e',' ','q','u','i','c','k',' ','b','r','o','w','n',' ','f', 'o','x',' ','j','u','m','p','s',' ','o', 'v','e', 'r',' ','t','h','e',' ','l','a','z','y',' ','d','o','g','.']\n",
    "len(sentence)\n",
    ">>>> 44\n",
    "```\n",
    "44 variables\n",
    "\n",
    "The model will be fed a certain amount of character and it will try to predict the next character as seen below:\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<i>The quick brown fox jumps over the lazy do </i>** [?]**\n",
    "\n",
    "The model should predict the letter **\"g\"**\n",
    "\n",
    "\n",
    "The model will not be case sensitive and it will not ignore punctuations. It will not read words but specific letters.It will also need an initial input phrase of 40 characters in order to generate text and from there its output will become its input.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "from keras.layers import LSTM,Input\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.utils.data_utils import get_file\n",
    "from keras.models import Model,load_model\n",
    "import numpy as np\n",
    "import random\n",
    "import sys\n",
    "import io"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Corpuses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So were going to try different corpuses and generate their own texts. Source:kaggle.com, gutenberg.org"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "corpus = ['shakespeare.txt',\n",
    "          'nietzsche.txt', \n",
    "          'emilydickensonpoems.txt',\n",
    "          'eminem.txt']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the length of each Corpuses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def total_char(corpus):\n",
    "    text = []\n",
    "    for c in corpus:\n",
    "        text_in_file = open(str(c),'r', encoding='ISO-8859-1') \n",
    "        text.append(text_in_file.read().lower()) \n",
    "        print('Total Characters in '+ str(c) + ': ' + str(len(text[-1])) )\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Characters in shakespeare.txt: 2588732\n",
      "Total Characters in nietzsche.txt: 600901\n",
      "Total Characters in emilydickensonpoems.txt: 170396\n",
      "Total Characters in eminem.txt: 739123\n"
     ]
    }
   ],
   "source": [
    "text_data = total_char(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count the unique charaters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def count_unique_char(corpus):\n",
    "    for c in range(len(corpus)):\n",
    "        chars = sorted(list(set(text_data[c])))\n",
    "        print('Total unique characters in '+str(corpus[c])+':', len(chars))\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total unique characters in shakespeare.txt: 41\n",
      "Total unique characters in nietzsche.txt: 59\n",
      "Total unique characters in emilydickensonpoems.txt: 52\n",
      "Total unique characters in eminem.txt: 72\n"
     ]
    }
   ],
   "source": [
    "count_unique_char(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we know the details of all the corpuses we will start working on just one corpus and then apply all the functions we have written to the other ones. For our initial corpus we will use shakespeare."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shakespeare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "shakespeare = text_data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a character dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ind_char(corpus):\n",
    "    chars = sorted(list(set(corpus)))\n",
    "    char_indices = dict((c, i) for i, c in enumerate(chars))\n",
    "    indices_char = dict((i, c) for i, c in enumerate(chars))\n",
    "        \n",
    "    return chars,char_indices,indices_char"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "chars, char_indices,indices_char = ind_char(shakespeare)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Character to Indices "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'\\n': 0,\n",
       " ' ': 1,\n",
       " '!': 2,\n",
       " '$': 3,\n",
       " '&': 4,\n",
       " \"'\": 5,\n",
       " ',': 6,\n",
       " '-': 7,\n",
       " '.': 8,\n",
       " '3': 9,\n",
       " ':': 10,\n",
       " ';': 11,\n",
       " '?': 12,\n",
       " '[': 13,\n",
       " ']': 14,\n",
       " 'a': 15,\n",
       " 'b': 16,\n",
       " 'c': 17,\n",
       " 'd': 18,\n",
       " 'e': 19,\n",
       " 'f': 20,\n",
       " 'g': 21,\n",
       " 'h': 22,\n",
       " 'i': 23,\n",
       " 'j': 24,\n",
       " 'k': 25,\n",
       " 'l': 26,\n",
       " 'm': 27,\n",
       " 'n': 28,\n",
       " 'o': 29,\n",
       " 'p': 30,\n",
       " 'q': 31,\n",
       " 'r': 32,\n",
       " 's': 33,\n",
       " 't': 34,\n",
       " 'u': 35,\n",
       " 'v': 36,\n",
       " 'w': 37,\n",
       " 'x': 38,\n",
       " 'y': 39,\n",
       " 'z': 40}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "char_indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Indices to character"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: '\\n',\n",
       " 1: ' ',\n",
       " 2: '!',\n",
       " 3: '$',\n",
       " 4: '&',\n",
       " 5: \"'\",\n",
       " 6: ',',\n",
       " 7: '-',\n",
       " 8: '.',\n",
       " 9: '3',\n",
       " 10: ':',\n",
       " 11: ';',\n",
       " 12: '?',\n",
       " 13: '[',\n",
       " 14: ']',\n",
       " 15: 'a',\n",
       " 16: 'b',\n",
       " 17: 'c',\n",
       " 18: 'd',\n",
       " 19: 'e',\n",
       " 20: 'f',\n",
       " 21: 'g',\n",
       " 22: 'h',\n",
       " 23: 'i',\n",
       " 24: 'j',\n",
       " 25: 'k',\n",
       " 26: 'l',\n",
       " 27: 'm',\n",
       " 28: 'n',\n",
       " 29: 'o',\n",
       " 30: 'p',\n",
       " 31: 'q',\n",
       " 32: 'r',\n",
       " 33: 's',\n",
       " 34: 't',\n",
       " 35: 'u',\n",
       " 36: 'v',\n",
       " 37: 'w',\n",
       " 38: 'x',\n",
       " 39: 'y',\n",
       " 40: 'z'}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indices_char"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cut the text in semi-redundant sequences of maxlen characters "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These will be used to feed the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "maxlen = 40\n",
    "step = 3\n",
    "def subset_sentences(text,maxlen,step,verbose=False):\n",
    "    #declare lists\n",
    "    sentences = []\n",
    "    next_chars = []\n",
    "    \n",
    "    #append subseted strings in to their respective lists\n",
    "    for i in range(0, len(text) - maxlen, step):\n",
    "        sentences.append(text[i: i + maxlen])\n",
    "        next_chars.append(text[i + maxlen])\n",
    "    if verbose == True: \n",
    "        print('sentence sequences:', len(sentences))\n",
    "    \n",
    "    return sentences,next_chars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentence sequences: 862898\n"
     ]
    }
   ],
   "source": [
    "sentences,next_chars = subset_sentences(shakespeare,maxlen,step,verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lines are now cut into a defined number of characters (Length of 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['before we proceed any further, hear me s',\n",
       " 'ore we proceed any further, hear me spea']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences[5:7]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next charater after each line are now assigned into a variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['p', 'k']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next_chars[5:7]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First Line:\n",
    "    \n",
    "    \n",
    "    \n",
    "**\"before we proceed any further, hear me s\"&nbsp;&nbsp;&nbsp;+&nbsp;&nbsp;&nbsp;\"p\"** &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<i>(full word is speak)</i>\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;sentence&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;next_chars\n",
    "\n",
    "\n",
    "### Second Line\n",
    "\n",
    "\n",
    "\n",
    "**\"ore we proceed any further, hear me spea\"&nbsp;&nbsp;&nbsp;+&nbsp;&nbsp;&nbsp;\"k\"** &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<i>(full word is speak)</i>\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;sentence&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;next_chars"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One hot encode x and y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def oh_xy(sentences,maxlen,chars,char_indices,next_chars):\n",
    "    #declare x,y into zero vectors\n",
    "    x = np.zeros((len(sentences), maxlen, len(chars)), dtype=np.bool)\n",
    "    y = np.zeros((len(sentences), len(chars)), dtype=np.bool)\n",
    "\n",
    "    #assign 1 in their appropriate spots in the matrix\n",
    "    for i, sentence in enumerate(sentences):\n",
    "        for t, char in enumerate(sentence):\n",
    "            x[i, t, char_indices[char]] = 1\n",
    "        y[i, char_indices[next_chars[i]]] = 1\n",
    "        \n",
    "    return x,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x,y = oh_xy(sentences,maxlen,chars,char_indices,next_chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 5786, 42712,   268,   213,     4,    11,  3478,   355,   390,\n",
       "         198,     4,  2662,   348,   450,     5,    37,    40,    47,\n",
       "          27,    20,    26,     6,     8,    21,    24,    40,    12,\n",
       "           3,   343,   151,   145,     8, 14628,  3399,  5113,  6847,\n",
       "       19465,  3511,  4330, 10216, 14941,   543,  3392,  7967,  6402,\n",
       "       12396, 15327,  3413,   110,  8489, 11435, 17238,  7192,  1490,\n",
       "        4524,   172,  5704,   234,    16,    10,     0,     1,     0,\n",
       "           1,     2,     1,     1,     0,     0,     0,     6,     3])"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building an LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_model(nodes,maxlen,chars):\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(nodes, input_shape=(maxlen, len(chars))))\n",
    "    model.add(Dense(len(chars)))\n",
    "    model.add(Activation('softmax'))\n",
    "    \n",
    "    optimizer = RMSprop(lr=0.01)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=optimizer)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sampling function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we increase the probability of the most probable words, and decrease the probabilities of less probable ones using **temperature**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sample(preds, temperature=1.0):\n",
    "    # helper function to sample an index from a probability array\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds) / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "    return np.argmax(probas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Printing the Generated Text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to the model to output the generated text when we wish to so, we can see how well the model is doing and if it needs further training. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def print_output(model,length,text,maxlen,chars,char_indices,indices_char,generated = ''):\n",
    "    generated = generated.lower()\n",
    "    start_index = random.randint(0, len(text) - maxlen - 1)\n",
    "    \n",
    "    if len(generated) != maxlen and generated != '':\n",
    "        print('Input must be ' + str(maxlen) + ' characters long.')\n",
    "        print('Input is ' + str(len(generated)) + ' characters long')\n",
    "    else:\n",
    "        if generated == '':\n",
    "            sentence = text[start_index: start_index + maxlen]\n",
    "            generated += sentence\n",
    "        else:\n",
    "            sentence = generated\n",
    "                \n",
    "        for diversity in [0.01,0.2, 0.5,1]:\n",
    "            generated1 = generated\n",
    "            sentence1 = sentence\n",
    "            print()\n",
    "            print()\n",
    "            print('/---------------------------------- DIVERSITY: %f ----------------------------------/'% diversity)\n",
    "\n",
    "            print('----- Generating with seed: \"' + sentence1 + '\"')\n",
    "            print()\n",
    "            print()\n",
    "            sys.stdout.write(generated1)\n",
    "            \n",
    "            for i in range(length):\n",
    "                x_pred = np.zeros((1, maxlen, len(chars)))\n",
    "                for t, char in enumerate(sentence1):\n",
    "                    x_pred[0, t, char_indices[char]] = 1.\n",
    "\n",
    "                preds = model.predict(x_pred, verbose=0)[0]\n",
    "                next_index = sample(preds, diversity)\n",
    "                next_char = indices_char[next_index]\n",
    "\n",
    "                generated1 += next_char\n",
    "                sentence1 = sentence1[1:] + next_char\n",
    "\n",
    "                sys.stdout.write(next_char)\n",
    "                sys.stdout.flush()\n",
    "            print()\n",
    "        \n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "862898/862898 [==============================] - 118s 137us/step - loss: 1.8497\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x24da6c6d320>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = build_model(256,maxlen,chars)\n",
    "model.fit(x, y,\n",
    "          batch_size=512,\n",
    "          epochs=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Generation: One Epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "/---------------------------------- DIVERSITY: 0.010000 ----------------------------------/\n",
      "----- Generating with seed: \"chus:\n",
      "my lord, i hear.\n",
      "\n",
      "pericles:\n",
      "most h\"\n",
      "\n",
      "\n",
      "chus:\n",
      "my lord, i hear.\n",
      "\n",
      "pericles:\n",
      "most him and the beging of my bear the bear of my bear the bear of my bear the bear of my bear the bear of my bear the bear of my bear the bear of my bear the bear of my bear the bear of my bear the bear of my bear the bear of my bear the bear of my bear the bear of my bear the bear of my bear the bear of my bear the bear of my bear the bear of my bear the bear of my bear the bear of my bear the bear of\n",
      "\n",
      "\n",
      "/---------------------------------- DIVERSITY: 0.200000 ----------------------------------/\n",
      "----- Generating with seed: \"chus:\n",
      "my lord, i hear.\n",
      "\n",
      "pericles:\n",
      "most h\"\n",
      "\n",
      "\n",
      "chus:\n",
      "my lord, i hear.\n",
      "\n",
      "pericles:\n",
      "most him and the beging of my dear the bear the beging\n",
      "that i did the be the was and be the begit\n",
      "that a father of the back and master that i have be the baster\n",
      "and fair faith that i was a face the hath bear the be the hasted\n",
      "that i have the be the begit of my brother\n",
      "the brother of him and the bed the but and daughter\n",
      "that shall the begin of the beging of my believe\n",
      "the be the that i am and the bost an\n",
      "\n",
      "\n",
      "/---------------------------------- DIVERSITY: 0.500000 ----------------------------------/\n",
      "----- Generating with seed: \"chus:\n",
      "my lord, i hear.\n",
      "\n",
      "pericles:\n",
      "most h\"\n",
      "\n",
      "\n",
      "chus:\n",
      "my lord, i hear.\n",
      "\n",
      "pericles:\n",
      "most him a purt, and thine contents the rad\n",
      "from ever the daughter and the dear had from flaither:\n",
      "if you with your swarm and made and be the wares\n",
      "and famiturant to the word for the have be that hath him\n",
      "and are the bask man the canst of even his main\n",
      "and she is the but the hard and the love\n",
      "that confriar to do you have talk him antony.\n",
      "\n",
      "mark antony:\n",
      "the dear the compass and of your bear of its\n",
      "thou no\n",
      "\n",
      "\n",
      "/---------------------------------- DIVERSITY: 1.000000 ----------------------------------/\n",
      "----- Generating with seed: \"chus:\n",
      "my lord, i hear.\n",
      "\n",
      "pericles:\n",
      "most h\"\n",
      "\n",
      "\n",
      "chus:\n",
      "my lord, i hear.\n",
      "\n",
      "pericles:\n",
      "most him; i know do be wan sick he dospinious timy heavoutd\n",
      "my of the wearsdings is nomes\n",
      "and wrong stones: he's becaaul of us thou do famit\n",
      "fush monesty tall recame his fast intue baster?\n",
      "\n",
      "flawis:\n",
      "\n",
      "lando:\n",
      "you was by chargen you, coarly heavens,\n",
      "and mide of much made would compants.\n",
      "\n",
      "beates:\n",
      "'twere with a kince; but decause thou sheald\n",
      "brutus' and, and have read the bunitys,\n",
      "speak that about that?\n",
      "\n",
      "king\n"
     ]
    }
   ],
   "source": [
    "print_output(model,400,shakespeare,maxlen,chars,char_indices,indices_char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "862898/862898 [==============================] - 118s 137us/step - loss: 1.4990\n",
      "Epoch 2/3\n",
      "862898/862898 [==============================] - 118s 137us/step - loss: 1.4379\n",
      "Epoch 3/3\n",
      "862898/862898 [==============================] - 118s 137us/step - loss: 1.4097\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x24da6c6d240>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x, y,\n",
    "          batch_size=512,\n",
    "          epochs=3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Generation: Three Epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "/---------------------------------- DIVERSITY: 0.010000 ----------------------------------/\n",
      "----- Generating with seed: \"\n",
      "there wanteth but a mean to fill your s\"\n",
      "\n",
      "\n",
      "\n",
      "there wanteth but a mean to fill your son\n",
      "which were the world that i will be the strength,\n",
      "and the world that i will be so shall be the\n",
      "with the world i will be so so shall be the\n",
      "with the world of the world that i will see the streets,\n",
      "that will she will not the strength of the\n",
      "with the world of the world i will be the\n",
      "with the world of the world i will be the\n",
      "with the world of the world i will be so soul.\n",
      "\n",
      "cornwall:\n",
      "what shall i "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nickj\\AppData\\Local\\Continuum\\Anaconda3\\envs\\py35\\lib\\site-packages\\ipykernel_launcher.py:4: RuntimeWarning: divide by zero encountered in log\n",
      "  after removing the cwd from sys.path.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wil\n",
      "\n",
      "\n",
      "/---------------------------------- DIVERSITY: 0.200000 ----------------------------------/\n",
      "----- Generating with seed: \"\n",
      "there wanteth but a mean to fill your s\"\n",
      "\n",
      "\n",
      "\n",
      "there wanteth but a mean to fill your son\n",
      "will strike the strength of the will of the threel.\n",
      "\n",
      "lucentio:\n",
      "what shall i was the world that i will be so life\n",
      "the man in the shall be the such a threel.\n",
      "\n",
      "benedick:\n",
      "what shall i lies the ways that well be so soul.\n",
      "\n",
      "cornwall:\n",
      "i will not be not the world of thy loved,\n",
      "that i am the strength of the hands of the good\n",
      "that i will be the arm of the hands and soul.\n",
      "\n",
      "king richard ii:\n",
      "hark the sun and\n",
      "\n",
      "\n",
      "/---------------------------------- DIVERSITY: 0.500000 ----------------------------------/\n",
      "----- Generating with seed: \"\n",
      "there wanteth but a mean to fill your s\"\n",
      "\n",
      "\n",
      "\n",
      "there wanteth but a mean to fill your sorrow.\n",
      "\n",
      "capulet:\n",
      "har is the prince, that so so fall the life,\n",
      "her commanded shall sleep unto the foul cause.\n",
      "\n",
      "a charge; what i should not do you were between him\n",
      "by thy souls cassius herelong soul?\n",
      "\n",
      "cominius:\n",
      "you'le question this into hands in god.\n",
      "\n",
      "comen:\n",
      "and there there were such a words of my humble,\n",
      "they will flowed no more command cureful services,\n",
      "that hear most father that kill the ready.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "/---------------------------------- DIVERSITY: 1.000000 ----------------------------------/\n",
      "----- Generating with seed: \"\n",
      "there wanteth but a mean to fill your s\"\n",
      "\n",
      "\n",
      "\n",
      "there wanteth but a mean to fill your sirs,\n",
      "but when he liked at king cobletrip;\n",
      "and the very cair that new-gent him resencal,\n",
      "the hand: your well menasoly i time is gloved\n",
      "a wit. thy dream, seturs, addelling blows,\n",
      "which huth, stheph, and hapry in thee\n",
      "home.\n",
      "\n",
      "cleopatra:\n",
      "hither, with here man?\n",
      "\n",
      "pedulet:\n",
      "why, name i fellow?\n",
      "\n",
      "begaln:\n",
      "farewell, sows!'\n",
      "here my wooly i mine has too thus: suiton,\n",
      "and in my tapstes, serve himself hast note:\n",
      "f\n"
     ]
    }
   ],
   "source": [
    "print_output(model,400,shakespeare,maxlen,chars,char_indices,indices_char)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interestingly, even with just a few epochs our model  can already form words in somewhat correct spelling. As we train the model further its performance will only get better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Generating Text From Different Sources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I ran the model in varying amounts. Basically, it requires a lot of training which I will not be doing since it consumes too much time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['shakespeare.txt', 'nietzsche.txt', 'emilydickensonpoems.txt', 'eminem.txt']"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Putting everything into a single function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gen_model(data,nodes,maxlen = 40,step = 3):\n",
    "    chars, char_indices,indices_char = ind_char(data)\n",
    "    print(len(chars))\n",
    "    sentences,next_chars = subset_sentences(data,maxlen,step)\n",
    "    x,y = oh_xy(sentences,maxlen,chars,char_indices,next_chars)\n",
    "\n",
    "    model = build_model(nodes,maxlen,chars)\n",
    "    return model,x,y,maxlen,chars,char_indices,indices_char"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shakespeare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_model(nodes,maxlen,chars):\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(nodes, input_shape=(maxlen, len(chars))))\n",
    "    model.add(Dense(len(chars)))\n",
    "    model.add(Activation('softmax'))\n",
    "    \n",
    "    optimizer = RMSprop(lr=0.0001)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=optimizer)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41\n"
     ]
    }
   ],
   "source": [
    "model_shakespeare,x,y,maxlen_shakespeare,chars_shakespeare,char_indices_shakespeare,indices_char_shakespeare = gen_model(text_data[0],nodes = 256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.fit(x, y,\n",
    "          batch_size=256,\n",
    "          epochs=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.save('model_shakespeare.h5') \n",
    "model = load_model('model_shakespeare.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_shakespeare = load_model('model_shakespeare.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "/---------------------------------- DIVERSITY: 0.010000 ----------------------------------/\n",
      "----- Generating with seed: \" manners shall lie all in one or two men\"\n",
      "\n",
      "\n",
      " manners shall lie all in one or two menisers\n",
      "than him the condetion of my spilit,\n",
      "which marry he have hath here as hole and dear.\n",
      "\n",
      "king henry vii:\n",
      "i would i have the field to rich fronchind: and\n",
      "the truth of his face, from sayishin yea,\n",
      "to wis or here the raget will made of sir,\n",
      "for theiried earth all of the seased with him.\n",
      "\n",
      "duke vincentic:\n",
      ":\n",
      "what man's? have you? sir?\n",
      "\n",
      "servant:\n",
      "a black and the prayers; they can be saint,\n",
      "and save you, you are an else.\n",
      "\n",
      "caesar:\n",
      "where is this gone?\n",
      "the noble duke my name, or earth he loves me.\n",
      "\n",
      "lady \n",
      "\n",
      "\n",
      "/---------------------------------- DIVERSITY: 0.200000 ----------------------------------/\n",
      "----- Generating with seed: \" manners shall lie all in one or two men\"\n",
      "\n",
      "\n",
      " manners shall lie all in one or two menisers\n",
      "than him the commended the passing off\n",
      "all fortunes, we are his nobled, and the law\n",
      "liester of the way.\n",
      "\n",
      "cleopatra:\n",
      "i wilt not: sir, you say in all there warwick,\n",
      "and say the what i should i have but endue,\n",
      "that thou discatchant shall be he mite the\n",
      "detagry of her comes!\n",
      "\n",
      "king richard iii:\n",
      "nor, but tell me to you. come, look you,\n",
      "and fell these fave on stormed out awhied.\n",
      "\n",
      "benedick:\n",
      "so is a dovine. to him they love me.\n",
      "\n",
      "lord wither:\n",
      "they not for high one of leather's gaud,\n",
      "and the god dead\n",
      "\n",
      "\n",
      "/---------------------------------- DIVERSITY: 0.500000 ----------------------------------/\n",
      "----- Generating with seed: \" manners shall lie all in one or two men\"\n",
      "\n",
      "\n",
      " manners shall lie all in one or two men\n",
      "let them come. 'till there are too: farguise suchonges the straight.\n",
      "the king his spirits of the voldest angle;\n",
      "whose stands upon the prood of place,\n",
      "which he by light when her it for everk.\n",
      "\n",
      "lady anne:\n",
      "stack thou the rote, what, with the sword so?\n",
      "\n",
      "hert:\n",
      "no, cojelt your friends, to channe man are: the queen\n",
      "is edward's lady, the streng his our children\n",
      "you have a troway and endling thing there,\n",
      "and that the liked and changes of such a child.\n",
      "\n",
      "second gentleman:\n",
      "to the forsing morcher of man, sh\n",
      "\n",
      "\n",
      "/---------------------------------- DIVERSITY: 1.000000 ----------------------------------/\n",
      "----- Generating with seed: \" manners shall lie all in one or two men\"\n",
      "\n",
      "\n",
      " manners shall lie all in one or two menours hate;\n",
      "with her what marchs effect.\n",
      "\n",
      "cherpabor:\n",
      "or edge long anon; weltow. how to his many;\n",
      "and great might hour in his peckurior preveice im:\n",
      "bring of treet, cassam, what effest this is myself.\n",
      "\n",
      "gloucester:\n",
      "you have his victuonious base affurding\n",
      "that gaving him in light out take: are you strait\n",
      "that's lishough and old marriage-butch?\n",
      "prove, the mother was as brother-shops our parties; he moses all.\n",
      "\n",
      "katharina:\n",
      "i think miserve love, but thyselfur you:\n",
      "into the speech he shous have it;\n",
      "and f\n"
     ]
    }
   ],
   "source": [
    "print_output(model_shakespeare,500,text_data[0],maxlen_shakespeare,chars_shakespeare,char_indices_shakespeare,indices_char_shakespeare)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nietzche"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_model(nodes,maxlen,chars):\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(nodes, input_shape=(maxlen, len(chars)),return_sequences=True))\n",
    "    model.add(LSTM(nodes))\n",
    "    model.add(Dense(len(chars)))\n",
    "    model.add(Activation('softmax'))\n",
    "    \n",
    "    optimizer = RMSprop(lr=0.001)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=optimizer)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "59\n"
     ]
    }
   ],
   "source": [
    "model_nietzsche,x,y,maxlen_nietzsche,chars_nietzsche,char_indices_nietzsche,indices_char_nietzsche = gen_model(text_data[1],nodes = 256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.fit(x, y,\n",
    "          batch_size=256,\n",
    "          epochs=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.save('model_nietzsche.h5') \n",
    "model = load_model('model_nietzsche.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_nietzsche = load_model('model_nietzsche.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "/---------------------------------- DIVERSITY: 0.010000 ----------------------------------/\n",
      "----- Generating with seed: \"however,\n",
      "where it is believed that the l\"\n",
      "\n",
      "\n",
      "however,\n",
      "where it is believed that the lengua an"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nickj\\AppData\\Local\\Continuum\\Anaconda3\\envs\\py35\\lib\\site-packages\\ipykernel_launcher.py:4: RuntimeWarning: divide by zero encountered in log\n",
      "  after removing the cwd from sys.path.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d rank and finally\n",
      "believes that at last the lose of the rible, the french revess of man morality the\n",
      "trouble and arrogon of man, that which inner calling all misunderstanding and upon the individual about the rudence for tremplate in ourselves\n",
      "have good repules them, inasure, and after called themselves, in\n",
      "such a herdinic of every kind of the hontire of the reverleged\n",
      "his own doing insterence and certain of this guiltic certainty of pleasure of\n",
      "an idea of the spirit, as elsome, a state\n",
      "\n",
      "\n",
      "/---------------------------------- DIVERSITY: 0.200000 ----------------------------------/\n",
      "----- Generating with seed: \"however,\n",
      "where it is believed that the l\"\n",
      "\n",
      "\n",
      "however,\n",
      "where it is believed that the lengua and rank and finally\n",
      "believes that at last menrich, in the same least and like every things, there must alway discoverity of corncencess,\n",
      " and in the same laughter superior delight into of\n",
      "the polman sexual more soul of the end, in order to kind of men of the\n",
      "morality of the ancient tistic revenge for the\n",
      "bal, of his posses, and also belong must say\n",
      "befbece a yes?\n",
      "\n",
      "1yer had !it is hads not conference to his belief an act, botly, which is ! raties the tendency of moral value, and in a\n",
      "were \n",
      "\n",
      "\n",
      "/---------------------------------- DIVERSITY: 0.500000 ----------------------------------/\n",
      "----- Generating with seed: \"however,\n",
      "where it is believed that the l\"\n",
      "\n",
      "\n",
      "however,\n",
      "where it is believed that the lengua and rank of consequences morality. in the end of the discort of\n",
      "driftined as a thing opinion. the human of news health the\n",
      "last in usperior religions in which being retures here?--yy, owing the possibility of impulses reads not dis regarded by\n",
      "micknered with command. schopenhe is a serious and obligationsly\n",
      "or strength of will away from its ladful questively one of the\n",
      "point of view of the jews, and the laws elevates of their religious charded and risk, attents to\n",
      "a sumplicity of the intel\n",
      "\n",
      "\n",
      "/---------------------------------- DIVERSITY: 1.000000 ----------------------------------/\n",
      "----- Generating with seed: \"however,\n",
      "where it is believed that the l\"\n",
      "\n",
      "\n",
      "however,\n",
      "where it is believed that the lensure of being, i somates that\n",
      "man can no discers, he will himself on heart a rearly\n",
      "spiring and uncertain likerion, and who'er like their divinifes adorn the other hand, those\n",
      "delishing all standay of the universal philosophy.\n",
      "\n",
      "shoppesions contemporarily not obsome thought. let usulare\n",
      "with love beford usual seems when he has glouse\n",
      " nothise to us! perhaps they will not be miner everything through the superior of every\n",
      "upon the struggh instion, and who knows, what is effect, the short the skep\n"
     ]
    }
   ],
   "source": [
    "print_output(model_nietzsche,500,text_data[1],maxlen_nietzsche,chars_nietzsche,char_indices_nietzsche,indices_char_nietzsche)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Emily Dickenson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_model(nodes,maxlen,chars):\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(nodes, input_shape=(maxlen, len(chars)),return_sequences=True))\n",
    "    model.add(LSTM(nodes))\n",
    "    model.add(Dense(len(chars)))\n",
    "    model.add(Activation('softmax'))\n",
    "    \n",
    "    optimizer = RMSprop(lr=0.001)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=optimizer)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52\n"
     ]
    }
   ],
   "source": [
    "model_emilydickenson,x,y,maxlen_emilydickenson,chars_emilydickenson,char_indices_emilydickenson,indices_char_emilydickenson = gen_model(text_data[2],nodes = 256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.fit(x, y,\n",
    "          batch_size=256,\n",
    "          epochs=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.save('model_emilydickenson.h5') \n",
    "model = load_model('model_emilydickenson.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_emilydickenson = load_model('model_emilydickenson.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "/---------------------------------- DIVERSITY: 0.010000 ----------------------------------/\n",
      "----- Generating with seed: \"l rise\n",
      "when i shall be forgiven,\n",
      "till ha\"\n",
      "\n",
      "\n",
      "l rise\n",
      "when i shall be forgiven,\n",
      "till hair a little loo"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nickj\\AppData\\Local\\Continuum\\Anaconda3\\envs\\py35\\lib\\site-packages\\ipykernel_launcher.py:4: RuntimeWarning: divide by zero encountered in log\n",
      "  after removing the cwd from sys.path.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k to dee,\n",
      "  prayer the sem yought be\n",
      "the smiler away.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "xxxiii.\n",
      "\n",
      "lont the blook, --\n",
      "  ond the could not be one feel;\n",
      "i hought little keach wour the ,\n",
      "ay hear we preays.\n",
      "\n",
      "seakes your will like morning crown\n",
      "\n",
      "to child by jubt to greating in\n",
      "the speein was, miel-we play\n",
      "for not serasing of neirs.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "liii.\n",
      "\n",
      "a blaker dauter war low meen\n",
      "contion affience past\n",
      "condensed of the fanter stood,\n",
      "and midew and whoo it was brand,\n",
      "and then the eart her one.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "xv.\n",
      "\n",
      "the harting the morn ha\n",
      "\n",
      "\n",
      "/---------------------------------- DIVERSITY: 0.200000 ----------------------------------/\n",
      "----- Generating with seed: \"l rise\n",
      "when i shall be forgiven,\n",
      "till ha\"\n",
      "\n",
      "\n",
      "l rise\n",
      "when i shall be forgiven,\n",
      "till hair a little look to dee,\n",
      "  prayes un an adloon for his,\n",
      "hands must the sun he sun,\n",
      "some created, and waith beggagl.\n",
      "it known had would love go do ame,\n",
      "but horied in she on the himm\n",
      "  i hilf you.\n",
      "\n",
      "i 't you where '  my not her fringer\n",
      "                                                                                                                                                                                                                                                                           \n",
      "\n",
      "\n",
      "/---------------------------------- DIVERSITY: 0.500000 ----------------------------------/\n",
      "----- Generating with seed: \"l rise\n",
      "when i shall be forgiven,\n",
      "till ha\"\n",
      "\n",
      "\n",
      "l rise\n",
      "when i shall be forgiven,\n",
      "till hair am lived my simple sind, and hone with canses\n",
      "  i could not sexperce!\n",
      "\n",
      "befor the yould brook a burnave face\n",
      "i could not fing, --\n",
      "ond it the first penies,\n",
      "but sparres of beaker\n",
      "and the sert of life\n",
      "\n",
      "and they heart to me fing,\n",
      "and done her, low, her distersed ats,\n",
      "and then reture earld the singleg,\n",
      "  and such the sun a broad?\n",
      "'s raid:\n",
      "so i may the hills away\n",
      "  the glaves of the arrien!\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "liii.\n",
      "\n",
      "reducting.\n",
      "\n",
      "it midd a carrit bad,\n",
      "with thit the bodfer fres destone\n",
      "  is all more a hand gall,\n",
      "eth\n",
      "\n",
      "\n",
      "/---------------------------------- DIVERSITY: 1.000000 ----------------------------------/\n",
      "----- Generating with seed: \"l rise\n",
      "when i shall be forgiven,\n",
      "till ha\"\n",
      "\n",
      "\n",
      "l rise\n",
      "when i shall be forgiven,\n",
      "till hair am lived the morn,\n",
      "and his how softic times\n",
      "as its not of his.\n",
      "\n",
      "her allity in a faint of ream,\n",
      "  an each i'm imble allomet.\n",
      "thin trown the crowd have signing --\n",
      "the haves the such her smile --\n",
      "  not full i hear her lowar!\n",
      "             ummimon affering,\n",
      "                                                                                                                                                                                                            meee,\n",
      "lif mumin must by proveer.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "l.\n"
     ]
    }
   ],
   "source": [
    "print_output(model_emilydickenson,500,text_data[2],maxlen_emilydickenson,chars_emilydickenson,char_indices_emilydickenson,indices_char_emilydickenson)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eminem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_model(nodes,maxlen,chars):\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(nodes, input_shape=(maxlen, len(chars)),return_sequences=True))\n",
    "    model.add(LSTM(nodes))\n",
    "    model.add(Dense(len(chars)))\n",
    "    model.add(Activation('softmax'))\n",
    "    \n",
    "    optimizer = RMSprop(lr=0.001)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=optimizer)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "72\n"
     ]
    }
   ],
   "source": [
    "model_eminem,x,y,maxlen_eminem,chars_eminem,char_indices_eminem,indices_char_eminem = gen_model(text_data[3],nodes = 256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.fit(x, y,\n",
    "          batch_size=256,\n",
    "          epochs=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.save('model_eminem.h5') \n",
    "model = load_model('model_eminem.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_eminem = load_model('model_eminem.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "/---------------------------------- DIVERSITY: 0.010000 ----------------------------------/\n",
      "----- Generating with seed: \"p my window\n",
      "and i can't see at all\n",
      "and e\"\n",
      "\n",
      "\n",
      "p my window\n",
      "and i can't see at all\n",
      "and even if i c"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nickj\\AppData\\Local\\Continuum\\Anaconda3\\envs\\py35\\lib\\site-packages\\ipykernel_launcher.py:4: RuntimeWarning: divide by zero encountered in log\n",
      "  after removing the cwd from sys.path.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ould it'll all be gray\n",
      "put your picture on my wall\n",
      "it reminds me, that it's not so bad\n",
      "it's not so bad\n",
      "mee the rest pick's in the champ,\n",
      "i's make ma get clops (get a walk on the floor\n",
      "with me out of this shit on a wrong kits highs were to shit hout that but it on the mic man\n",
      "i'ma make you ain't got man\n",
      "i tried to stavil the truts\n",
      "from a bitch rap is dragged him\n",
      "on a nas suck, my dick's so fuck you minut\n",
      "think you fuckin' dis\n",
      "that i catt coming wates, i'd goin to race this mome out\n",
      "they\n",
      "\n",
      "\n",
      "/---------------------------------- DIVERSITY: 0.200000 ----------------------------------/\n",
      "----- Generating with seed: \"p my window\n",
      "and i can't see at all\n",
      "and e\"\n",
      "\n",
      "\n",
      "p my window\n",
      "and i can't see at all\n",
      "and even if i could it'll all be gray\n",
      "put your picture on my wall\n",
      "it reminds me, that it's not so bad\n",
      "it's not so bad\n",
      "mee the rest pitefth, shit what happen 'em\n",
      "in a there and persencing me back out of the white for deals\n",
      "(ed hell)\n",
      "or everything, i mare dog, i'm stripped in a bang for the way while i stop his crack as a huggous the chocket up on cime\n",
      "then you're my face, i flond me through the game\n",
      "wit the hood up the whole hells you're just to\n",
      "see i dream i used to fash motherfuckin' gun\n",
      "'cause fuck\n",
      "\n",
      "\n",
      "/---------------------------------- DIVERSITY: 0.500000 ----------------------------------/\n",
      "----- Generating with seed: \"p my window\n",
      "and i can't see at all\n",
      "and e\"\n",
      "\n",
      "\n",
      "p my window\n",
      "and i can't see at all\n",
      "and even if i could it'll all be gray\n",
      "put your picture on my wall\n",
      "it reminds me, that it's not so bad\n",
      "it's not so bad\n",
      "mee the rest pitefth, shit what happen\n",
      "show you through the fuck you talking me come and play\n",
      "maybe you account you-ause at that changin you walk, he gets talk on the corcarders\n",
      "and i annigh, i'm wonger in a cabo, when i'm ever like (o' like thisug))))\n",
      "fe'll hold you whack around and grinds\n",
      "and it's no lone booms i don't got that fuck you that rap on the up\n",
      "no sep, these she's be sign\n",
      "\n",
      "\n",
      "/---------------------------------- DIVERSITY: 1.000000 ----------------------------------/\n",
      "----- Generating with seed: \"p my window\n",
      "and i can't see at all\n",
      "and e\"\n",
      "\n",
      "\n",
      "p my window\n",
      "and i can't see at all\n",
      "and even if i could it'll all be gray\n",
      "put your picture on my wall\n",
      "it reminds me, that it's not so bad\n",
      "it's not so bad\n",
      "mee the rest piteme, we can hear it all drugs to dis\n",
      "can't painnta caz hugs, you gon' do it only knee just to get sore\n",
      "skinns let one da cime walk around driving a cilch rol\n",
      "we look fucked up in your obdinil with my adding\n",
      "but i'm the kingol and the mother saids i miss\n",
      "that'll stack my appacture and dibble right that man, ha ha\n",
      "im brongy the policece!\n",
      "mama put it flar, shady slap that\n"
     ]
    }
   ],
   "source": [
    "print_output(model_eminem,500,text_data[3],maxlen_eminem,chars_eminem,char_indices_eminem,indices_char_eminem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
